{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7054-12-300-l_drucker_se_su_st_st_512_32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert directory structure from synthnet evaluation pipeline format to Huggingface format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load a local dataset from image directory (huggingface)\n",
    "# https://huggingface.co/docs/datasets/image_dataset\n",
    "DATASET_TRAIN_NAME = \"visda2017\"\n",
    "DS_TRAIN_DIR = f\"/home/dritter/projects/evaluation-pipeline/data/{DATASET_TRAIN_NAME}/train/\"\n",
    "OUT_ROOT_TRAIN = f\"data/{DATASET_TRAIN_NAME}\"\n",
    "# DS_TRAIN_DIR = f'/home/dennis/Desktop/work/evaluation_pipeline_data/visda/train/{DATASET_TRAIN_NAME}'\n",
    "# load the our data dir (evaluation pipeline format)\n",
    "# change directory structure\n",
    "#   from:   data/ds_name/images/class/mesh/images\n",
    "#   to:     data/ds_name/images/class/images\n",
    "for path, dns, fns in os.walk(DS_TRAIN_DIR):\n",
    "    for fn in fns:\n",
    "        split_path = path.split(\"/\")\n",
    "        label = split_path[-2]\n",
    "        split = \"train\"\n",
    "        os.makedirs(f\"{OUT_ROOT_TRAIN}/{label}\", exist_ok=True)\n",
    "        shutil.copy(f\"{path}/{fn}\", f\"{OUT_ROOT_TRAIN}/{split}/{label}/{fn}\")\n",
    "\n",
    "# load cifar10 (only small portion for demonstration purposes)\n",
    "# train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "# # split up training into training + validation\n",
    "# splits = train_ds.train_test_split(test_size=0.1)\n",
    "# train_ds = splits['train']\n",
    "# val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load a local dataset from image directory (huggingface)\n",
    "# https://huggingface.co/docs/datasets/image_dataset\n",
    "DATASET_TEST_NAME = \"visda2017\"\n",
    "DS_TRAIN_DIR = f\"data/{DATASET_TRAIN_NAME}/train\"\n",
    "DS_TEST_DIR = f\"/home/dritter/projects/evaluation-pipeline/data/datasets/{DATASET_TEST_NAME}/val\"\n",
    "OUT_ROOT_TEST = f\"data/{DATASET_TEST_NAME}/val\"\n",
    "\n",
    "# Get classes present in train dataset\n",
    "TRAIN_CLASSES = os.listdir(\"data/visda2017/train\")\n",
    "print(TRAIN_CLASSES)\n",
    "\n",
    "for path, dns, fns in os.walk(DS_TEST_DIR):\n",
    "    for fn in fns:\n",
    "        split_path = path.split(\"/\")\n",
    "        label = split_path[-1]\n",
    "        if label in TRAIN_CLASSES:\n",
    "            os.makedirs(f\"{OUT_ROOT_TEST}/{label}\", exist_ok=True)\n",
    "            shutil.copy(f\"{path}/{fn}\", f\"{OUT_ROOT_TEST}/{label}/{fn}\")\n",
    "\n",
    "# load cifar10 (only small portion for demonstration purposes)\n",
    "# train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:2000]'])\n",
    "# # split up training into training + validation\n",
    "# splits = train_ds.train_test_split(test_size=0.1)\n",
    "# train_ds = splits['train']\n",
    "# val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN_NAME = \"7054-12-300-l_drucker_se_su_st_st_512_32\"\n",
    "DATASET_TEST_NAME = \"topex-real-123_pb_256\"\n",
    "TRAIN_CLASSES = os.listdir(f\"data/{DATASET_TRAIN_NAME}\")\n",
    "TEST_CLASSES = os.listdir(f\"data/{DATASET_TEST_NAME}\")\n",
    "\n",
    "for c in TEST_CLASSES:\n",
    "    if c not in TRAIN_CLASSES:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, 100):\n",
    "    X.append(f\"img_{i}\")\n",
    "    y.append(f\"label_{i%5}\")\n",
    "print(X)\n",
    "print(y)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN_i:\", train_index)\n",
    "    print(\"TEST_i:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "print(\"TRAIN\", (X_train, y_train))\n",
    "print(\"TEST\", (X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visda2017 test set (assign labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data = \"data/visda2017/val\"\n",
    "\n",
    "for path, dns, fns in os.walk(data):\n",
    "    for fn in fns:\n",
    "        fn_split = fn.split(\"_\")\n",
    "        fn_split[1] = fn_split[1][:-4]\n",
    "        new_fn = \"_\".join(fn_split)\n",
    "        shutil.move(f\"{path}/{fn}\", f\"{path}/{new_fn}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visda2017 MeshGrid\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def grid(imps, grid_size=(4, 4), cell_size=(64, 64), padding=0):\n",
    "    ims = [Image.open(imp) for imp in imps]\n",
    "    transform = transforms.Compose([transforms.Resize(cell_size), transforms.PILToTensor()])\n",
    "\n",
    "    n_img = grid_size[0] * grid_size[1]\n",
    "    ims_tensor = [transform(im) for im in ims]\n",
    "    grid = make_grid(ims_tensor, nrow=grid_size[1], padding=padding)\n",
    "    im_grid = torchvision.transforms.ToPILImage()(grid)\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "data = \"/home/dritter/projects/evaluation-pipeline/data/datasets/visda2017/train/images\"\n",
    "out = \"data/visda2017_meshgrid\"\n",
    "\n",
    "d = {}\n",
    "for path, dns, fns in os.walk(data):\n",
    "    for fn in fns:\n",
    "        pathsplit = path.split(\"/\")\n",
    "        label = pathsplit[-2]\n",
    "\n",
    "        fnsplit = fn.split(\"__\")\n",
    "        mesh = fnsplit[0]\n",
    "        angles = fnsplit[1].split(\"_\")\n",
    "        light_angle = angles[1]\n",
    "        if f\"{label}_{mesh}_{light_angle}\" in d.keys():\n",
    "            d[f\"{label}_{mesh}_{light_angle}\"].append(f\"{path}/{fn}\")\n",
    "        else:\n",
    "            d[f\"{label}_{mesh}_{light_angle}\"] = [f\"{path}/{fn}\"]\n",
    "\n",
    "for key, val in d.items():\n",
    "    val.sort()\n",
    "    imgrid = grid(val[:16], cell_size=(48, 48))\n",
    "    imgrid = ImageOps.expand(imgrid, border=(16, 16, 16, 16), fill=\"white\")\n",
    "    label = key.split(\"_\")[0]\n",
    "    out_dir = f\"{out}/{label}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    imgrid.save(f\"{out_dir}/{key}_train.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelNet10 Meshgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def grid(imps, grid_size=(4, 4), cell_size=(64, 64), padding=0):\n",
    "    ims = [Image.open(imp) for imp in imps]\n",
    "    transform = transforms.Compose([transforms.Resize(cell_size), transforms.PILToTensor()])\n",
    "\n",
    "    n_img = grid_size[0] * grid_size[1]\n",
    "    ims_tensor = [transform(im) for im in ims]\n",
    "    grid = make_grid(ims_tensor, nrow=grid_size[1], padding=padding)\n",
    "    im_grid = torchvision.transforms.ToPILImage()(grid)\n",
    "    return im_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "data = \"data/modelnet10\"\n",
    "out = \"data/modelnet10meshgrid\"\n",
    "\n",
    "d = {}\n",
    "white_bg = Image.new(\"RGBA\", (224, 224), (255, 255, 255, 255))\n",
    "for path, dns, fns in os.walk(data):\n",
    "    for fn in fns:\n",
    "        pathsplit = path.split(\"/\")\n",
    "        split = pathsplit[-2]\n",
    "        label = pathsplit[-1]\n",
    "\n",
    "        fnsplit = fn.split(\"_\")\n",
    "        mesh = fnsplit[-2]\n",
    "        meshimage = fnsplit[-1].split(\".\")[0]\n",
    "        key = f\"{split}_{label}_{mesh}\"\n",
    "        if key in d.keys():\n",
    "            d[key].append(f\"{path}/{fn}\")\n",
    "        else:\n",
    "            d[key] = [f\"{path}/{fn}\"]\n",
    "\n",
    "for key, val in d.items():\n",
    "    val.sort()\n",
    "    imgrid = grid(val[1::2], cell_size=(48, 48))\n",
    "    imgrid = ImageOps.expand(imgrid, border=(16, 16, 16, 16), fill=\"white\")\n",
    "    ksplit = key.split(\"_\")\n",
    "    split = ksplit[0]\n",
    "    label = ksplit[1]\n",
    "    if label == \"night\":\n",
    "        label += f\"_{ksplit[2]}\"\n",
    "    mesh = ksplit[-1]\n",
    "    out_dir = f\"{out}/{split}/{label}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    imgrid = Image.alpha_composite(white_bg, imgrid)\n",
    "    imgrid.save(f\"{out_dir}/{split}_{label}_{mesh}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-synthnet-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a29b6eddd4b2efc6a6aab00861817de7f57901e05d1d0c07d1240390f1e332c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
