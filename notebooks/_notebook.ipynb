{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_modules\n",
    "import torch\n",
    "from data_modules.baseline_finetune_dm import BaselineFinetuneDM\n",
    "from data_modules.generic_finetune_dm import GenericFinetuneDM\n",
    "\n",
    "dm = GenericFinetuneDM(train_dir=\"data/topex-printer/train/\", test_dir=\"data/topex-printer/test/\")\n",
    "# gftdm = GenericFinetuneDM()\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "train_dataloader = dm.train_dataloader()\n",
    "val_dataloader = dm.val_dataloader()\n",
    "test_dataloader = dm.test_dataloader()\n",
    "\n",
    "inputs, labels = next(iter(train_dataloader))\n",
    "print(dm.num_classes)\n",
    "print(len(dm.test.classes))\n",
    "print(dm.test.classes == dm.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = \"/home/dennis/projects/evaluation-pipeline/data/datasets/visda2017_meshes/train\"\n",
    "out = \"/home/dennis/projects/evaluation-pipeline/data/datasets/visda2017_meshgrid\"\n",
    "\n",
    "d = {}\n",
    "for path, dns, fns in os.walk(data):\n",
    "\n",
    "    for fn in fns:\n",
    "\n",
    "        fnsplit = fn.split(\"__\")\n",
    "        mesh = fnsplit[0]\n",
    "        angles = fnsplit[1].split(\"_\")\n",
    "        light_angle = angles[1]\n",
    "        if f\"{mesh}_{light_angle}\" in d.keys():\n",
    "            d[f\"{mesh}_{light_angle}\"].append(fn)\n",
    "        else:\n",
    "            d[f\"{mesh}_{light_angle}\"] = [fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# TODO: Parse visda2017/train and use dictionary to make grids\n",
    "\n",
    "img_dir = \"data/visda2017/train/car\"\n",
    "\n",
    "grid_size = (7, 7)\n",
    "padding = 0\n",
    "n_img = grid_size[0] * grid_size[1]\n",
    "renders = os.listdir(img_dir)\n",
    "rnd_model = random.sample(renders, k=1)[0]\n",
    "rnd_model = rnd_model.split(\"_\")[3]\n",
    "renders = [render for render in renders if render.split(\"_\")[3] == rnd_model]\n",
    "rnd_imgs = random.sample(renders, k=n_img)\n",
    "imgs = [read_image(f\"{img_dir}/{img_fn}\") for img_fn in rnd_imgs]\n",
    "grid = make_grid(imgs, nrow=grid_size[1], padding=padding)\n",
    "img_grid = torchvision.transforms.ToPILImage()(grid)\n",
    "img_grid = img_grid.resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "img_dir = \"data/visda2017/train/car\"\n",
    "\n",
    "grid_size = (7, 7)\n",
    "padding = 0\n",
    "n_img = grid_size[0] * grid_size[1]\n",
    "renders = os.listdir(img_dir)\n",
    "rnd_model = random.sample(renders, k=1)[0]\n",
    "rnd_model = rnd_model.split(\"_\")[3]\n",
    "renders = [render for render in renders if render.split(\"_\")[3] == rnd_model]\n",
    "rnd_imgs = random.sample(renders, k=n_img)\n",
    "imgs = [read_image(f\"{img_dir}/{img_fn}\") for img_fn in rnd_imgs]\n",
    "grid = make_grid(imgs, nrow=grid_size[1], padding=padding)\n",
    "img_grid = torchvision.transforms.ToPILImage()(grid)\n",
    "img_grid = img_grid.resize((224, 224))\n",
    "img_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Data Collator for Huggingface image classification Dataset\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A batch of data samples\n",
    "\n",
    "    Returns:\n",
    "        dict: prepared batch of data samples. Stacked pixel values and tensor of labels\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39-synthnet-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a29b6eddd4b2efc6a6aab00861817de7f57901e05d1d0c07d1240390f1e332c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
